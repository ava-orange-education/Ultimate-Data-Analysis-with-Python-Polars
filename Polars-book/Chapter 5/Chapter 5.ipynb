{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5aa598-3132-46e5-a42d-072080863e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl \n",
    "\n",
    "df = pl.read_csv(\"electricity_consumption.csv\")\n",
    "\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a13ae9f-961c-4966-a87b-8ca440ee3d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the dataset\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True)\n",
    "\n",
    "# Calculate the average consumption per city\n",
    "average_consumption = df.group_by(\"City\").agg(pl.col(\"Consumption (kWh)\").mean().alias(\"Avg Consumption (kWh)\"))\n",
    "\n",
    "# Display the result\n",
    "print(average_consumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39be428a-e5e5-432c-92ff-ec571aa99daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the dataset\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True).with_columns(pl.col(\"Date\").dt.week().alias(\"Week_Number\"))\n",
    "\n",
    "# Group by City and calculate required metrics\n",
    "city_stats = df.group_by(\"City\").agg([\n",
    "    pl.col(\"Consumption (kWh)\").mean().alias(\"Avg Consumption (kWh)\"),\n",
    "    pl.col(\"Peak Hours Usage (kWh)\").max().alias(\"Max Peak Hour Usage (kWh)\")\n",
    "])\n",
    "\n",
    "# Display the result\n",
    "print(city_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb47478-59d4-47ea-9bbe-798a92faba20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the dataset\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True)\n",
    "\n",
    "# Group by City and calculate required metrics\n",
    "city_stats = df.group_by(\"City\").agg([\n",
    "    pl.col(\"Consumption (kWh)\").mean().alias(\"Avg Consumption (kWh)\"),\n",
    "    pl.col(\"Peak Hours Usage (kWh)\").max().alias(\"Max Peak Hour Usage (kWh)\"),\n",
    "    pl.col(\"Temperature (째C)\").min().alias(\"Min Temperature (째C)\"),\n",
    "    pl.col(\"Temperature (째C)\").max().alias(\"Max Temperature (째C)\")\n",
    "])\n",
    "\n",
    "# Display the result\n",
    "print(city_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bc37a-9213-4f25-981f-8fc3702afdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load dataset\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True)\n",
    "\n",
    "# Define a custom function to calculate Peak Hour Load Factor\n",
    "def calculate_peak_hour_load_factor(df):\n",
    "    total_consumption = df[\"Consumption (kWh)\"].sum()\n",
    "    max_peak_hour_usage = df[\"Peak Hours Usage (kWh)\"].max()\n",
    "    num_days = df[\"Date\"].n_unique()  # Count unique days in the data\n",
    "    peak_hour_load_factor = total_consumption / (num_days * max_peak_hour_usage) if max_peak_hour_usage > 0 else None\n",
    "    return pl.DataFrame({\"Peak Hour Load Factor\": [peak_hour_load_factor]})\n",
    "\n",
    "# Apply custom function using groupby_map\n",
    "city_stats = df.group_by(\"City\").map_groups(calculate_peak_hour_load_factor)\n",
    "\n",
    "# Display results\n",
    "print(city_stats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc74f842-b48e-407c-b20c-c75a210e82f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the dataset\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True).with_columns(pl.col(\"Date\").dt.week().alias(\"Week_Number\"))\n",
    "\n",
    "# Group by City and calculate required metrics\n",
    "city_stats = df.group_by(\"City\",\"Week_Number\").agg([\n",
    "    pl.col(\"Consumption (kWh)\").mean().alias(\"Avg Consumption (kWh)\"),\n",
    "    pl.col(\"Peak Hours Usage (kWh)\").max().alias(\"Max Peak Hour Usage (kWh)\")\n",
    "])\n",
    "\n",
    "# Display the result\n",
    "print(city_stats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4456120-2b80-4647-8be8-623068a17af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "states = pl.DataFrame({\n",
    "    \"City\": [\n",
    "        \"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\",\n",
    "        \"Philadelphia\", \"San Antonio\", \"San Diego\", \"Dallas\", \"San Jose\"\n",
    "    ],\n",
    "    \"State\": [\n",
    "        \"New York\", \"California\", \"Illinois\", \"Texas\", \"Arizona\",\n",
    "        \"Pennsylvania\", \"Texas\", \"California\", \"Texas\", \"California\"\n",
    "    ]\n",
    "})\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True)\n",
    "\n",
    "# Perform a FULL OUTER JOIN on \"City\"\n",
    "merged_df = df.join(states, on=\"City\", how=\"inner\")\n",
    "\n",
    "# Display the result\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fcb2e3-e71b-47a3-b00c-b383cd0e419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the two CSV files\n",
    "df_2025 = pl.read_csv(\"electricity_consumption_2025.csv\")\n",
    "print(\"2025 dataset row count\" , df_2025.shape[0])\n",
    "df_previous = pl.read_csv(\"electricity_consumption.csv\")\n",
    "print(\"2024 dataset row count\" , df_previous.shape[0])\n",
    "\n",
    "# Concatenate the DataFrames vertically (stacking rows)\n",
    "concatenated_df = pl.concat([df_2025, df_previous])\n",
    "\n",
    "# Display the first few rows of the concatenated DataFrame\n",
    "print(\"concatenated df row count\" , concatenated_df.shape[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4a2689-74cf-4c14-ad9b-197324585705",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Example DataFrames\n",
    "df1 = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n",
    "    \"Consumption\": [5000, 4500, 6000]\n",
    "})\n",
    "\n",
    "df2 = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n",
    "    \"Temperature\": [30, 28, 25]\n",
    "})\n",
    "\n",
    "df3 = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n",
    "    \"Peak Hours Usage\": [2500, 2200, 2800]\n",
    "})\n",
    "\n",
    "# Merge the DataFrames\n",
    "merged_df = df1.join(df2, on=\"City\", how=\"inner\") \\\n",
    "               .join(df3, on=\"City\", how=\"inner\")\n",
    "\n",
    "# Display the merged DataFrame\n",
    "print(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904d1417-a94d-4cc7-af46-fcf56c48df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Create first DataFrame (First 5 Days)\n",
    "df1 = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n",
    "    \"2025-01-01\": [500, 450, 600],\n",
    "    \"2025-01-02\": [520, 460, 620],\n",
    "    \"2025-01-03\": [530, 470, 630],\n",
    "    \"2025-01-04\": [540, 480, 640],\n",
    "    \"2025-01-05\": [550, 490, 650],\n",
    "})\n",
    "\n",
    "# Create second DataFrame (Next 5 Days)\n",
    "df2 = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n",
    "    \"2025-01-06\": [560, 500, 660],\n",
    "    \"2025-01-07\": [570, 510, 670],\n",
    "    \"2025-01-08\": [580, 520, 680],\n",
    "    \"2025-01-09\": [590, 530, 690],\n",
    "    \"2025-01-10\": [600, 540, 700],\n",
    "})\n",
    "\n",
    "# Horizontally stack the two DataFrames\n",
    "hstacked_df = df1.hstack(df2.drop(\"City\"))\n",
    "\n",
    "# Display the result\n",
    "print(hstacked_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5cd985-f883-4fae-b0bb-8e4a88d00020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the dataset\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True)\n",
    "\n",
    "# Pivot the data: City as index, Dates as columns, and Consumption as values\n",
    "pivot_df = df.pivot(\n",
    "    index=\"City\",\n",
    "    on=\"Date\",\n",
    "    values=\"Consumption (kWh)\"\n",
    ")\n",
    "\n",
    "# Display the pivoted DataFrame\n",
    "print(pivot_df)\n",
    "\n",
    "# Save the pivoted DataFrame to a new CSV file\n",
    "pivot_df.write_csv(\"pivoted_electricity_consumption.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e49c5a-4380-41e6-9c81-cc5a635e1f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Unpivot the data: Convert multiple date columns into \"Date\" and \"Consumption (kWh)\"\n",
    "melted_df = pivot_df.unpivot(\n",
    "    index=[\"City\"],      # Columns to keep unchanged\n",
    "    on=pivot_df.columns[1:],  # Columns to unpivot (all date columns)\n",
    "    variable_name=\"Date\",        # New column for dates\n",
    "    value_name=\"Consumption (kWh)\"  # New column for consumption values\n",
    ")\n",
    "\n",
    "# Display the unpivoted DataFrame\n",
    "print(melted_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1128f93-7b7a-41c8-9e0b-b415149bf838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Load the dataset and parse dates\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True)\n",
    "\n",
    "# Sort the DataFrame by Date before resampling\n",
    "df_sorted = df.sort(\"Date\")\n",
    "\n",
    "# Convert daily consumption data into weekly totals\n",
    "weekly_consumption = df_sorted.group_by_dynamic(\"Date\", every=\"1w\").agg(pl.col(\"Consumption (kWh)\").sum())\n",
    "\n",
    "# Display the weekly aggregated consumption\n",
    "print(weekly_consumption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf966215-8ac1-45f2-aa3d-4fc7a4654cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort(\"Date\")  # Ensure data is sorted by time\n",
    "df_upsampled = df_sorted.with_columns(\n",
    "    pl.col(\"Consumption (kWh)\").interpolate()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876c2126-0212-4627-9641-e2d9f19f5b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_upsampled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aef906-b3cc-4561-aebe-55cdbf04d37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125d61b4-92db-4ccc-80e5-ac974de5c69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset and parse dates\n",
    "df = pl.read_csv(\"electricity_consumption.csv\", try_parse_dates=True)\n",
    "print(\"original dataframe\", df.shape)\n",
    "\n",
    "# Sort the DataFrame by Date before any operation\n",
    "df_sorted = df.sort(\"Date\")\n",
    "\n",
    "# Delete random rows before performing interpolation\n",
    "random_indices = np.random.choice(df_sorted.height, 50, replace=False)\n",
    "df_without_random_dates = df_sorted.filter(~pl.col(\"Date\").is_in(df_sorted[\"Date\"][random_indices]))\n",
    "\n",
    "print(\"modified dataframe with random samples removed\",df_without_random_dates)\n",
    "# Perform interpolation on the remaining data\n",
    "df_upsampled = df_without_random_dates.upsample(time_column=\"Date\",every=\"1d\",group_by =\"City\",maintain_order=True).select(pl.all().forward_fill())\n",
    "\n",
    "# Display the upsampled data\n",
    "print(\"upsampled dataframe\" , df_upsampled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf93771-e26e-47af-8528-62f311ef67ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample DataFrame\n",
    "df = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"],\n",
    "    \"Consumption (kWh)\": [450, 600, 350, 500]\n",
    "})\n",
    "\n",
    "# Apply a transformation: Scaling the consumption values by 1.2\n",
    "df_transformed = df.with_columns(\n",
    "    (pl.col(\"Consumption (kWh)\") * 1.2).alias(\"Scaled Consumption\")\n",
    ")\n",
    "\n",
    "print(df_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724c671a-da6c-49a9-b30c-5efb892b0453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n",
    "})\n",
    "\n",
    "# Define a dictionary for mapping cities to state codes\n",
    "city_to_state = {\n",
    "    \"New York\": \"NY\",\n",
    "    \"Los Angeles\": \"CA\",\n",
    "    \"Chicago\": \"IL\",\n",
    "    \"Houston\": \"TX\"\n",
    "}\n",
    "\n",
    "# Map the cities to state codes using the apply method\n",
    "df_with_states = df.with_columns(\n",
    "    pl.col(\"City\").map_elements(lambda x: city_to_state.get(x, \"Unknown\"),return_dtype=pl.Utf8).alias(\"State\")\n",
    ")\n",
    "\n",
    "print(df_with_states)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48699ed8-7ecb-43d7-b148-7832f3f8194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample DataFrame\n",
    "df = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\"]\n",
    "})\n",
    "print(df.dtypes,\" <--- default data types\")\n",
    "\n",
    "# Convert the 'City' column to a categorical type\n",
    "df_categorized = df.with_columns(pl.col(\"City\").cast(pl.Categorical).alias(\"City Categorical\"))\n",
    "\n",
    "print(df_categorized.dtypes,\" <--- Categorical data types\")\n",
    "\n",
    "# One-hot encode the categorical column\n",
    "df_one_hot = df_categorized.to_dummies()\n",
    "\n",
    "print(df_one_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491def69-e887-49c4-858d-25eaaec3c4ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Create a sample stock price dataset with a list of specific dates\n",
    "df = pl.DataFrame({\n",
    "    \"Date\": [\n",
    "        \"2024-01-01\", \"2024-01-03\", \"2024-01-05\", \"2024-01-06\", \"2024-01-08\",\n",
    "        \"2024-01-10\", \"2024-01-12\", \"2024-01-15\", \"2024-01-17\", \"2024-01-20\"\n",
    "    ],\n",
    "    \"Stock_Price\": [100, 102, 98, 105, 110, 108, 112, 115, 118, 120]\n",
    "})\n",
    "\n",
    "# Convert the Date column to datetime format\n",
    "df = df.with_columns(pl.col(\"Date\").str.to_date())\n",
    "\n",
    "# Compute 3-day rolling average of stock prices\n",
    "df = df.with_columns(\n",
    "    pl.col(\"Stock_Price\").rolling_mean(window_size=3).alias(\"Rolling_Avg_3d\")\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14025608-9196-49c4-93e1-4243fceaf51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Sample dataset: Daily stock prices\n",
    "df = pl.DataFrame({\n",
    "    \"Date\": [\"2024-01-01\", \"2024-01-02\", \"2024-01-03\", \"2024-01-04\", \"2024-01-05\",\n",
    "             \"2024-01-06\", \"2024-01-07\", \"2024-01-08\", \"2024-01-09\", \"2024-01-10\"],\n",
    "    \"Stock_Price\": [100, 102, 98, 105, 110, 108, 112, 115, 118, 120]\n",
    "},schema={\"Date\": pl.Date, \"Stock_Price\": pl.Int32})\n",
    "\n",
    "# Compute a 5-day rolling mean using expr.rolling_mean()\n",
    "df = df.with_columns(\n",
    "    pl.mean(\"Stock_Price\").rolling(index_column=\"Date\", period=\"3d\").alias(\"Rolling_Avg_3d\")\n",
    ")\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e339f2e-e4ca-40da-91cf-249e8b3d73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "# Create a sample dataset for car production\n",
    "df = pl.DataFrame({\n",
    "    \"Date\": [\n",
    "        \"2024-01-01\", \"2024-01-03\", \"2024-01-05\", \"2024-01-06\", \"2024-01-08\",\n",
    "        \"2024-01-10\", \"2024-01-12\", \"2024-01-15\", \"2024-01-17\", \"2024-01-20\"\n",
    "    ],\n",
    "    \"Cars_Produced\": [200, 250, 180, 300, 320, 400, 350, 370, 390, 420]\n",
    "})\n",
    "\n",
    "\n",
    "# Compute cumulative sum of cars produced\n",
    "df = df.with_columns(\n",
    "    pl.col(\"Cars_Produced\").cum_sum().alias(\"Total_Cars_Produced\")\n",
    ")\n",
    "\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7309017-edca-49d8-904e-54e24142e047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Sample patient visit data\n",
    "df = pl.DataFrame({\n",
    "    \"Hospital\": [\"H1\", \"H1\", \"H1\", \"H2\", \"H2\", \"H2\"],\n",
    "    \"Patient_ID\": [101, 102, 103, 201, 202, 203],\n",
    "    \"Visit_Date\": [\"2024-02-01\", \"2024-02-03\", \"2024-02-05\", \"2024-02-02\", \"2024-02-04\", \"2024-02-06\"]\n",
    "}).with_columns(pl.col(\"Visit_Date\").str.to_date())\n",
    "\n",
    "# Rank patients based on visit order within each hospital\n",
    "df = df.with_columns(\n",
    "    pl.col(\"Visit_Date\").rank().over(\"Hospital\", order_by=\"Visit_Date\").alias(\"Visit_Rank\")\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c58e5bf-509e-4c27-a68f-53616c9ce1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Sample electricity consumption data\n",
    "df = pl.DataFrame({\n",
    "    \"City\": [\"NY\", \"NY\", \"NY\", \"LA\", \"LA\", \"LA\"],\n",
    "    \"Date\": [\"2024-03-01\", \"2024-03-01\", \"2024-03-01\", \"2024-03-01\", \"2024-03-01\", \"2024-03-01\"],\n",
    "    \"Hour\": [1, 2, 3, 1, 2, 3],\n",
    "    \"Power_Usage\": [500, 600, 550, 700, 750, 720]\n",
    "}).with_columns(pl.col(\"Date\").str.to_date())\n",
    "\n",
    "# Compute max hourly power usage per city per day\n",
    "df = df.with_columns(\n",
    "    pl.col(\"Power_Usage\").max().over([\"City\", \"Date\"]).alias(\"Peak_Hourly_Usage\")\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14347099-b982-43b8-a038-f8614797cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "\n",
    "# Generate a larger sample size (50 cities with random power usage values)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "cities = [f\"City_{i}\" for i in range(1, 51)]  # 50 cities\n",
    "power_usage = np.random.randint(100, 1500, size=50)  # Random power usage between 100 and 1500\n",
    "\n",
    "# Add one extreme value to simulate an outlier\n",
    "power_usage[-1] = 15000  # Set last value as an outlier\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pl.DataFrame({\n",
    "    \"City\": cities,\n",
    "    \"Power_Usage\": power_usage\n",
    "})\n",
    "\n",
    "# Calculate Q1, Q3, and IQR\n",
    "Q1 = df[\"Power_Usage\"].quantile(0.25)\n",
    "Q3 = df[\"Power_Usage\"].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Determine outlier thresholds\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "# Flag outliers (values outside the threshold range)\n",
    "df = df.with_columns(\n",
    "    pl.when(pl.col(\"Power_Usage\") < lower_bound)\n",
    "    .then(True)\n",
    "    .otherwise(pl.when(pl.col(\"Power_Usage\") > upper_bound).then(True).otherwise(False))\n",
    "    .alias(\"Is_Outlier\")\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17690229-49b5-4bf7-bcb2-104416779f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Create a DataFrame with date-time data\n",
    "df = pl.DataFrame({\n",
    "    \"Timestamp\": [\"2024-01-01 12:30:00\", \"2024-01-02 14:45:00\", \"2024-01-03 16:15:00\"]\n",
    "}).with_columns(pl.col(\"Timestamp\").str.strptime(pl.Datetime).alias(\"Date\"))\n",
    "\n",
    "# Extracting year, month, day, and hour\n",
    "df = df.with_columns([\n",
    "    pl.col(\"Date\").dt.year().alias(\"Year\"),\n",
    "    pl.col(\"Date\").dt.month().alias(\"Month\"),\n",
    "    pl.col(\"Date\").dt.day().alias(\"Day\"),\n",
    "    pl.col(\"Date\").dt.hour().alias(\"Hour\")\n",
    "])\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651078c4-b938-43fe-aff0-f68e1a992677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Create a DataFrame with time-zone-aware data\n",
    "df = pl.DataFrame({\n",
    "    \"Timestamp\": [\"2024-01-01 12:30:00+05:30\", \"2024-01-02 14:45:00+05:30\", \"2024-01-03 16:15:00+05:30\"]\n",
    "}).with_columns(pl.col(\"Timestamp\").str.strptime(pl.Datetime, \"%Y-%m-%d %H:%M:%S%z\").alias(\"Date\"))\n",
    "\n",
    "# Convert timestamps to UTC\n",
    "df = df.with_columns(pl.col(\"Date\").dt.convert_time_zone(\"UTC\").alias(\"Date_UTC\"))\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185a80a-5f63-45e5-8223-888048ea84c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Sample data with numerical values\n",
    "df = pl.DataFrame({\n",
    "    \"Age\": [18, 25, 40, 55, 60, 72, 85, 90, 32, 41]\n",
    "})\n",
    "\n",
    "# Define bin edges (age ranges)\n",
    "bins = [0, 20, 40, 60, 80, 100]\n",
    "\n",
    "# Create a new column \"Age_Bucket\" by binning the \"Age\" column\n",
    "df = df.with_columns(\n",
    "    pl.col(\"Age\").cut(bins).alias(\"Age_Bucket\")\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b40549-257e-43b2-b43e-d40283a2c496",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.DataFrame({\"salary\": [30000, 45000, 60000, 75000, 100000, 120000]})\n",
    "df.with_columns(\n",
    "    pl.col(\"salary\").qcut([0.25, 0.75], labels=[\"low\", \"med\", \"high\"]).alias(\"qcut\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c9544-c41f-40ea-851d-84b07bf6147a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Sample data with nested structures (lists and structs)\n",
    "df = pl.DataFrame({\n",
    "    \"City\": [\"New York\", \"Los Angeles\", \"Chicago\"],\n",
    "    \"Weather\": [\n",
    "        {\"Temperature\": 25, \"Humidity\": 65},\n",
    "        {\"Temperature\": 28, \"Humidity\": 70},\n",
    "        {\"Temperature\": 20, \"Humidity\": 75}\n",
    "    ],\n",
    "    \"Hourly_Temperature\": [\n",
    "        [22, 23, 24, 25],\n",
    "        [27, 28, 29, 30],\n",
    "        [19, 20, 21, 22]\n",
    "    ]\n",
    "})\n",
    "print(\"Original DF\")\n",
    "print(df)\n",
    "# Flatten the nested structs in the 'Weather' column\n",
    "df_flattened = df.with_columns(\n",
    "    pl.col(\"Weather\").struct.field(\"Temperature\").alias(\"Temperature\"),\n",
    "    pl.col(\"Weather\").struct.field(\"Humidity\").alias(\"Humidity\")\n",
    ")\n",
    "\n",
    "# Flatten the 'Hourly_Temperature' list into separate rows using explode\n",
    "df_exploded = df_flattened.explode(\"Hourly_Temperature\")\n",
    "\n",
    "# Show the final flattened DataFrame\n",
    "print(\"flattened DF\")\n",
    "print(df_exploded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b7a8fd-c895-4412-9af5-eb733294f9e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59db63-6cb2-4150-a880-ee6bda3babbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
